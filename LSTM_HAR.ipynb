{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by activity\n",
    "def data_by_activity(X, y, activities):\n",
    "    # group windows by activity\n",
    "    grouped=list()\n",
    "    #return {a:X[y[:,0]==a] for a in activities}\n",
    "    for i in activities:\n",
    "        ac = X[Y[:,0]==i]\n",
    "        grouped.append(ac)\n",
    "        #print(i,ac.shape)\n",
    "\n",
    "def load_all_data(directory):\n",
    "    # Load\n",
    "    filename=\"mHealth_subject1.csv\"\n",
    "    df = pd.read_csv(directory+filename)\n",
    "    df.insert(0, 'id', 1)\n",
    "\n",
    "\n",
    "    for i in range(9):\n",
    "        number=str(i+2)\n",
    "        filename=\"mHealth_subject\"+number+\".csv\"\n",
    "    #   print(directory+filename)\n",
    "        df_subject = pd.read_csv(directory+filename)\n",
    "        df_subject.insert(0, 'id', i+2)\n",
    "        df = df.append(df_subject)\n",
    "\n",
    "\n",
    "\n",
    "    # Cleaning\n",
    "    df =df.query('label != 0')\n",
    "    raw = df\n",
    "    \n",
    "    # Separate data\n",
    "    X = df.iloc[:, :24]\n",
    "    Y = df.iloc[:,24]\n",
    "\n",
    "    return raw,X,Y\n",
    "\n",
    "\n",
    "def class_breakdown(data):\n",
    "    # convert the numpy array into a dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    # group data by the class value and calculate the number of rows\n",
    "    counts = df.groupby(0).size()\n",
    "    # retrieve raw rows\n",
    "    counts = counts.values\n",
    "    # summarize\n",
    "    for i in range(len(counts)):\n",
    "        percent = counts[i] / len(df) * 100\n",
    "        print('Class=%d, total=%d, percentage=%.3f' % (i + 1, counts[i], percent))\n",
    "\n",
    "\n",
    "# Method to convert data to series\n",
    "def to_series(data, off, activity_list, subject_id):\n",
    "    subject_data = data.query('id==' + str(subject_id))\n",
    "    series = [[]]\n",
    "    for activity in activity_list:\n",
    "        ser = np.asmatrix(subject_data.query(\"label==\"+str(activity)).iloc[:, off]).T\n",
    "        series=np.append(series,ser)\n",
    "    return series\n",
    "\n",
    "\n",
    "def min_max_normalization(X):\n",
    "    row,columns=X.shape\n",
    "    for i in range(columns):\n",
    "        v = X[:, i]  \n",
    "        X[:, i] = (v - v.min()) / ((v.max() - v.min()) if (v.max() - v.min())!=0 else 1)\n",
    "    return X\n",
    "\n",
    "# Scale dataset in a range\n",
    "def range_normalization(X,a,b):\n",
    "    row,columns=X.shape\n",
    "    for i in range(columns):\n",
    "        v = X[:, i]  \n",
    "        X[:, i] = (b-a)*((v - v.min()) / ((v.max() - v.min()) if (v.max() - v.min())!=0 else 1))+a\n",
    "    return X\n",
    "\n",
    "\n",
    "# Create a dataset as a time sequence\n",
    "def create_dataset(X, y, time_steps=1, step=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        labels = y.iloc[i: i + time_steps]\n",
    "        Xs.append(v)        \n",
    "        ys.append(stats.mode(labels)[0][0])\n",
    "    return np.array(Xs), np.array(ys).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Encode Target as a vector\n",
    "def encode_target(Y):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    enc = enc.fit(Y)\n",
    "    encode_target = enc.transform(Y)\n",
    "    return encode_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(values):\n",
    "    return values * (1 - values)\n",
    "\n",
    "def tanh_normal(values):\n",
    "    return np.tanh(values)\n",
    "\n",
    "def tanh_derivative(values):\n",
    "    return 1. - values ** 2\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)\n",
    "    exp_X = exp_X/exp_X_sum\n",
    "    return exp_X\n",
    "\n",
    "\n",
    "# createst uniform random array w/ values in [a,b) and shape args\n",
    "def rand_arr(a, b, *args):\n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(*args) * (b - a) + a\n",
    "\n",
    "\n",
    "# Creates a list of tuples wit mini batches for X and Y\n",
    "def get_mini_batches(X, y, batch_size):\n",
    "    random_idxs = np.random.choice(len(y), len(y), replace=False)\n",
    "    X_shuffled = X[random_idxs, :, :]\n",
    "    y_shuffled = y[random_idxs,:]\n",
    "    mini_batches = [(X_shuffled[i:i + batch_size, :,:], y_shuffled[i:i + batch_size,:]) for i in range(0, len(y), batch_size)]\n",
    "    return mini_batches\n",
    "\n",
    "def get_index_batch(batch,range_low, range_high):\n",
    "    seedValue = (random.randrange(sys.maxsize))//2**32\n",
    "    np.random.seed(seedValue)\n",
    "    return np.random.randint(low=range_low, high=range_high, size=(batch,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class params will contain all parameters of the network \n",
    "#import numpy as np\n",
    "\n",
    "\n",
    "class lstm_param:\n",
    "\n",
    "    def __init__(self, units, x_dim, classes):\n",
    "        self.units = units\n",
    "        self.x_dim = x_dim\n",
    "        concat_len = x_dim + units\n",
    "        # weight matrices\n",
    "        self.wg = rand_arr(-0.1, 0.1, units, concat_len)\n",
    "        self.wi = rand_arr(-0.1, 0.1, units, concat_len)\n",
    "        self.wf = rand_arr(-0.1, 0.1, units, concat_len)\n",
    "        self.wo = rand_arr(-0.1, 0.1, units, concat_len)\n",
    "        self.wk = rand_arr(-0.1, 0.1, units, classes)\n",
    "        \n",
    "        # bias terms\n",
    "        self.bg = rand_arr(-0.1, 0.1, units,  1)\n",
    "        self.bi = rand_arr(-0.1, 0.1, units,  1)\n",
    "        self.bf = rand_arr(-0.1, 0.1, units,  1)\n",
    "        self.bo = rand_arr(-0.1, 0.1, units,  1)\n",
    "        self.bk = rand_arr(-0.1, 0.1, classes,1)\n",
    "\n",
    "        # diffs (derivative of loss function w.r.t. all parameters)\n",
    "        self.wg_diff = np.zeros((units, concat_len))\n",
    "        self.wi_diff = np.zeros((units, concat_len))\n",
    "        self.wf_diff = np.zeros((units, concat_len))\n",
    "        self.wo_diff = np.zeros((units, concat_len))\n",
    "        self.wk_diff = np.zeros((units, classes))\n",
    "        \n",
    "        self.bg_diff = np.zeros((units,  1))\n",
    "        self.bi_diff = np.zeros((units,  1))\n",
    "        self.bf_diff = np.zeros((units,  1))\n",
    "        self.bo_diff = np.zeros((units,  1))\n",
    "        self.bk_diff = np.zeros((classes,1))\n",
    "\n",
    "    def apply_diff(self, lr=1):\n",
    "        self.wg -= lr * self.wg_diff\n",
    "        self.wi -= lr * self.wi_diff\n",
    "        self.wf -= lr * self.wf_diff\n",
    "        self.wo -= lr * self.wo_diff\n",
    "        self.wk -= lr * self.wk_diff\n",
    "        \n",
    "        self.bg -= lr * self.bg_diff\n",
    "        self.bi -= lr * self.bi_diff\n",
    "        self.bf -= lr * self.bf_diff\n",
    "        self.bo -= lr * self.bo_diff\n",
    "        self.bk -= lr * self.bk_diff\n",
    "        \n",
    "        # reset diffs to zero\n",
    "        self.wg_diff = np.zeros_like(self.wg)\n",
    "        self.wi_diff = np.zeros_like(self.wi)\n",
    "        self.wf_diff = np.zeros_like(self.wf)\n",
    "        self.wo_diff = np.zeros_like(self.wo)\n",
    "        self.wk_diff = np.zeros_like(self.wk)\n",
    "        \n",
    "        self.bg_diff = np.zeros_like(self.bg)\n",
    "        self.bi_diff = np.zeros_like(self.bi)\n",
    "        self.bf_diff = np.zeros_like(self.bf)\n",
    "        self.bo_diff = np.zeros_like(self.bo)\n",
    "        self.bk_diff = np.zeros_like(self.bk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_state:\n",
    "    def __init__(self, units, x_dim,series):\n",
    "        self.cell_temp_values = np.zeros(units)\n",
    "        self.input_values     = np.zeros(units)\n",
    "        self.forget_values    = np.zeros(units)\n",
    "        self.output_values    = np.zeros(units)\n",
    "        self.cell_values      = np.zeros(units)\n",
    "        self.h                = np.zeros((units,series))\n",
    "        \n",
    "        self.diff_h_values    = np.zeros_like(self.h)\n",
    "        self.diff_cell_values = np.zeros_like(self.cell_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lstm_param import lstm_param\n",
    "\n",
    "class lstm_model:\n",
    "\n",
    "    def __init__(self,features, target, classes, units, batch):\n",
    "        self.X = features\n",
    "        self.Y = target\n",
    "        self.classes = classes\n",
    "        self.units = units\n",
    "        self.batch_size = batch\n",
    "\n",
    "        z,x,y=features.shape\n",
    "\n",
    "        self.x_dim = x\n",
    "        self.h_dim = x+units\n",
    "        self.params = lstm_param(units,x,classes)\n",
    "               \n",
    "\n",
    "    def train(self, epochs):\n",
    "        samples, features, series=self.X.shape\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # create batches\n",
    "            batch_set = get_mini_batches(self.X,self.Y,self.batch_size)\n",
    "            cell_prev = None\n",
    "            h_prev    = None\n",
    "            \n",
    "            # LSTM recurrent\n",
    "            state = lstm_state(self.units, self.x_dim, series)\n",
    "            \n",
    "            \n",
    "            # Iterations over batchs\n",
    "            for current_batch in batch_set:\n",
    "                x_current = current_batch[0]\n",
    "                y_current = current_batch[1]\n",
    "                \n",
    "            \n",
    "                # Cell state\n",
    "                if cell_prev is None: cell_prev = np.zeros_like(state.cell_values)\n",
    "                if h_prev is None: h_prev = np.zeros_like(state.h)  \n",
    "                loss = 0\n",
    "                avg_loss = 0\n",
    "                predicted_values = np.zeros((samples,2,1))\n",
    "                \n",
    "                \n",
    "                # Iterations by value\n",
    "                for step in range(samples):\n",
    "                    time_serie = x_current[step]\n",
    "                    target = np.matrix(y_current[step])\n",
    "                \n",
    "                    # Forward\n",
    "                    predicted, step_loss, state = self.forward_step(time_serie, target, h_prev, cell_prev, state)\n",
    "                    predicted_values[step] = predicted\n",
    "                    loss += step_loss\n",
    "                    \n",
    "                    h_prev = state.h\n",
    "                    cell_prev =  state.cell_values\n",
    "                    break\n",
    "                    \n",
    "                # Back\n",
    "                avg_loss = loss/self.batch_size \n",
    "                \n",
    "                # TODO: \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                break\n",
    "            break\n",
    "        \n",
    "                        \n",
    "            \n",
    "        \n",
    "                \n",
    "    # x = time series matrix\n",
    "    # h_prev = hidden\n",
    "    def forward_step(self, x, target, h_prev, cell_prev, state):\n",
    "        prob=1\n",
    "        \n",
    "        #LSTM\n",
    "        xc = np.vstack((x, h_prev))\n",
    "        state.cell_temp_values = np.tanh(np.dot(self.params.wg, xc) + self.params.bg)\n",
    "        state.input_values = sigmoid(np.dot(self.params.wi, xc) + self.params.bi)\n",
    "        state.forget_values = sigmoid(np.dot(self.params.wf, xc) + self.params.bf)\n",
    "        state.output_values = sigmoid(np.dot(self.params.wo, xc) + self.params.bo)\n",
    "        state.cell_values = state.cell_temp_values * state.input_values + h_prev * state.forget_values\n",
    "        state.h = state.cell_values * state.output_values\n",
    "        \n",
    "        # Classification\n",
    "        predicted = softmax(np.dot(self.params.wk.T,state.h)+self.params.bk)\n",
    "        prob = np.multiply(prob,np.sum(np.multiply(target.T,predicted),axis=1).reshape(-1,1))\n",
    "        result = np.sum(prob,axis=1,dtype=\"float32\")\n",
    "        \n",
    "        # Cross entropy loss\n",
    "        loss = (-1)*(np.sum((np.dot(target,np.log(predicted)) + np.dot(1-target,np.log(1-predicted))),axis=1).reshape(-1,1))\n",
    "\n",
    "        return result, loss, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "\n",
    "# Variable to save the number of steps in the sequence\n",
    "TIME_STEPS = 128\n",
    "# Step to recolect samples every STEP times a new sample of TIME_STEP will be recolected\n",
    "STEP = 40\n",
    "\n",
    "# Units in NN\n",
    "UNITS = 100\n",
    "\n",
    "# No. of classes for classification\n",
    "CLASSES = 2\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liliana/.local/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before encode:\n",
      "Train\n",
      "(1072, 128, 9) (1072, 1)\n",
      "Test\n",
      "(458, 128, 9) (458, 1)\n",
      "After encode:\n",
      "Train\n",
      "(1072, 128, 9) (1072, 2)\n",
      "Test\n",
      "(458, 128, 9) (458, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "directory=\"Data/MHEALTHDATASET/\"\n",
    "\n",
    "raw,X,y=load_all_data(directory)\n",
    "raw.head()\n",
    "\n",
    "# to beginning we are only going to use accelerometer\n",
    "valid_activities_set=raw.query(\"label==1 or label==4\") #raw.copy()\n",
    "valid_activities_set.columns\n",
    "\n",
    "# Get only 2 activities and accelerometer data\n",
    "# data=valid_activities_set[['id','acc_chest_x'    , 'acc_chest_y'    , 'acc_chest_z','label']].copy()\n",
    "data=valid_activities_set[['id','acc_chest_x'    , 'acc_chest_y'    , 'acc_chest_z',\n",
    "                               'acc_left_ank_x' , 'acc_left_ank_y' , 'acc_left_ank_z',\n",
    "                               'acc_right_arm_x', 'acc_right_arm_y', 'acc_right_arm_z','label']].copy()\n",
    "\n",
    "\n",
    "# Separate in train and test\n",
    "df_train = data[data['id'] <= 7]\n",
    "df_test = data[data['id'] > 7]\n",
    "\n",
    "\n",
    "# Scale data [-1,1] with min_max\n",
    "scale_columns = ['acc_chest_x'    , 'acc_chest_y'    , 'acc_chest_z',\n",
    "                               'acc_left_ank_x' , 'acc_left_ank_y' , 'acc_left_ank_z',\n",
    "                               'acc_right_arm_x', 'acc_right_arm_y', 'acc_right_arm_z']\n",
    "\n",
    "df_train.loc[:, scale_columns] = range_normalization(df_train[scale_columns].to_numpy(),-1,1)\n",
    "df_test.loc[:, scale_columns] = range_normalization(df_test[scale_columns].to_numpy(),-1,1)\n",
    "\n",
    "\n",
    "# Create dataset as time series\n",
    "X_train, y_train = create_dataset(\n",
    "    df_train[['acc_chest_x'    , 'acc_chest_y'    , 'acc_chest_z',\n",
    "                               'acc_left_ank_x' , 'acc_left_ank_y' , 'acc_left_ank_z',\n",
    "                               'acc_right_arm_x', 'acc_right_arm_y', 'acc_right_arm_z']], \n",
    "    df_train.label, \n",
    "    TIME_STEPS, \n",
    "    STEP\n",
    ")\n",
    "\n",
    "X_test, y_test = create_dataset(\n",
    "    df_test[['acc_chest_x'    , 'acc_chest_y'    , 'acc_chest_z',\n",
    "                               'acc_left_ank_x' , 'acc_left_ank_y' , 'acc_left_ank_z',\n",
    "                               'acc_right_arm_x', 'acc_right_arm_y', 'acc_right_arm_z']], \n",
    "    df_test.label, \n",
    "    TIME_STEPS, \n",
    "    STEP\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Before encode:\")\n",
    "print(\"Train\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"Test\")\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Encode Target\n",
    "y_train = encode_target(y_train)\n",
    "y_test = encode_target(y_test)\n",
    "\n",
    "print(\"After encode:\")\n",
    "print(\"Train\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"Test\")\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.83791584]]\n"
     ]
    }
   ],
   "source": [
    "har_lst=lstm_model(X_train, y_train, CLASSES, UNITS, BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "har_lst.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 228)\n"
     ]
    }
   ],
   "source": [
    "print(har_lst.params.wo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
